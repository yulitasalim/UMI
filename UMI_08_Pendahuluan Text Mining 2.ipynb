{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://tau-data.id/umi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img alt=\"\" src=\"images/0_Cover.jpg\"/></center> \n",
    "\n",
    "## <center><font color=\"blue\">Modul 08: Pendahuluan NLP & Text Preprocessing II</font></center>\n",
    "<b><center>(C) Taufik Sutanto - 2019</center>\n",
    "<center>tau-data Indonesia ~ https://tau-data.id ~ taufik@tau-data.id</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font color=\"blue\">Pendahuluan NLP & Text Preprocessing II: Data Crawling & Sentiment Analysis</font></center>\n",
    "<img alt=\"\" src=\"images/PDS_logo.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color=\"blue\">Workshop Schedule</font>\n",
    "\n",
    "## <font color=\"green\">Hari ke-4 (Kamis, 30 Jan 2020)</font>\n",
    "\n",
    "**Pendahuluan Natural Language Processing & Text PreProcessing**\n",
    "* 09:00 – 11:00 \tText Preprocessing\n",
    "* 11:00 – 12:00\tCrawling, Streaming, Scraping\n",
    "* 13:00 – 14:00\tText Analytics\n",
    "* 14:00 – 15.00\tSentiment Analysis\n",
    "* 15:00 – 16.00\tLatihan Text Analytics dan Sentiment Analysis \n",
    "\n",
    "Studi Kasus: **Text Analytics data media sosial perbincangan agama di media sosial**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Modules for Google Colab\n",
    "!wget https://raw.githubusercontent.com/taufikedys/UMI/master/taudata.py\n",
    "!mkdir data\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UMI/master/data/Tweets.json\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UMI/master/data/dataTweet.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UMI/master/data/kataNegID.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UMI/master/data/kataPosID.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UMI/master/data/slang.dic\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UMI/master/data/stopwords_id.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UMI/master/data/stopwords_en.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UMI/master/data/all_indo_man_tag_corpus_model.crf.tagger\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UMI/master/data/kata_dasar.txt\n",
    "!pip install unidecode\n",
    "!pip install pyLDAvis\n",
    "!pip install textblob\n",
    "!pip install sastrawi\n",
    "!pip install twython\n",
    "!pip install tweepy\n",
    "!pip install spacy\n",
    "!pip install python-crfsuite\n",
    "!python -m spacy download en\n",
    "!python -m spacy download xx\n",
    "!python -m spacy download en_core_web_sm\n",
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import taudata as tau, seaborn as sns; sns.set()\n",
    "import tweepy, json, nltk, urllib.request\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from twython import TwythonStreamer  \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"blue\">Outline Sentiment Analysis :</font>\n",
    "\n",
    "* Corpus-Based Sentiment Analysis\n",
    "* Metode Supervised untuk Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/9_Sentiment_Analysis_Meme.jpg\" style=\"height:300px; width:400px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Apakah sentiment analysis?</strong></p>\n",
    "\n",
    "<p>Sentiment Analysis adalah suatu proses komputasi untuk menentukan apakah suatu penrnyataan bermakna positive, negative, atau netral.</p>\n",
    "\n",
    "<p>Terkadang disebut juga sebagai&nbsp;<strong>opinion mining.</strong></p>\n",
    "\n",
    "<p><strong>Contoh aplikasi Sentiment Analysis</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li><strong>Business: tanggapan konsumen atas suatu produk</strong>.</li>\n",
    "\t<li><strong>Politics:&nbsp;</strong>Sentimen masyarakat sebagai strategi pemenangan pemilu/pilkada.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/9_SA_techniques.jpg\" style=\"height:300px; width:536px\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexicon Based berdasarkan \n",
    "# pattern = https://www.clips.uantwerpen.be/pages/pattern-en#sentiment\n",
    "Sentence = \"I love Bakpia so much it's delicious\"\n",
    "testimonial = TextBlob(Sentence)\n",
    "print(testimonial.sentiment)\n",
    "print('Polarity=Sentimen =', testimonial.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagaimana Dengan Bahasa Indonesia?\n",
    "<p>[A simple trick]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat = 'coto makassar mantep tenan euy'\n",
    "K = TextBlob(kalimat).translate(to='en')\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SenSub_ID(kalimat):\n",
    "    K = TextBlob(kalimat).translate(to='en')\n",
    "    pol,sub = K.sentiment\n",
    "    if pol>0:\n",
    "        pol='positive'\n",
    "    elif pol<0:\n",
    "        pol='negative'\n",
    "    else:\n",
    "        pol = 'netral'\n",
    "    if sub>0.5:\n",
    "        sub = 'Subjektif'\n",
    "    else:\n",
    "        sub = \"Objektif\"\n",
    "    return pol, sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat = 'makan coto makassar pakai jeruk nipis enak tenan'\n",
    "SenSub_ID(kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "# Warning, mungkin lambat karena membentuk model classifier* terlebih dahulu.\n",
    "# *Berdasarkan NLTK corpus ==> Language dependent\n",
    "Sentence = \"Textblob is amazingly simple to use\"\n",
    "blob = TextBlob(Sentence, analyzer=NaiveBayesAnalyzer())\n",
    "blob.sentiment\n",
    "# Good Explanation: https://medium.com/nlpython/sentiment-analysis-analysis-ee5da4448e37\n",
    "# Output probabilitas prediksinya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagaimana dengan Sentiment Analysis menggunakan NBC untuk Bahasa indonesia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "def bentukClassifier(P, Ne, Nt):\n",
    "    positive_features = [(word_feats(pos), 'pos') for pos in P]\n",
    "    negative_features = [(word_feats(neg), 'neg') for neg in Ne]\n",
    "    neutral_features = [(word_feats(neu), 'neu') for neu in Nt]\n",
    "    train_set = negative_features + positive_features + neutral_features\n",
    "    return NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "def prediksiSentiment(sentence, model):\n",
    "    pos, neg = 0, 0\n",
    "    words = sentence.lower().split(' ')\n",
    "    for word in words:\n",
    "        classResult = model.classify( word_feats(word))\n",
    "        if classResult == 'neg':\n",
    "            neg = neg + 1\n",
    "        if classResult == 'pos':\n",
    "            pos = pos + 1\n",
    "    if pos>neg:\n",
    "        return 'positif'\n",
    "    elif pos<neg:\n",
    "        return 'negatif'\n",
    "    else:\n",
    "        return 'netral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [ 'keren', 'suka', 'cinta', 'bagus', 'mantap', 'sadis', 'top']\n",
    "emot_pos = [':)', ':D']\n",
    "Ne = [ 'jelek', 'benci','buruk', 'najis', ':(']\n",
    "emot_neg = [':(', \":'(\"]\n",
    "\n",
    "Nt = [ 'bakso','film','pisang','pagi','makan','kopi','minum','sambil','abis']\n",
    "\n",
    "sentence = \"makan pempek minumnya teh panas, mantap sekali\"\n",
    "model = bentukClassifier(P+emot_pos, Ne+emot_neg, Nt) \n",
    "prediksiSentiment(sentence,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negasi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Negasi = ['tidak', 'ngga', 'engga', 'enggak', 'ndak', 'endak', 'tdk']\n",
    "Pos = P + [n +' '+ ne for n,ne in zip(Negasi, Ne)] + emot_pos\n",
    "Neg = Ne + [n +' '+ po for n,po in zip(Negasi, P)] + emot_neg\n",
    "\n",
    "print(Pos, Neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pos = [t.strip() for t in tau.LoadDocuments(file = 'data/kataPosID.txt')[0]]\n",
    "Neg = [t.strip() for t in tau.LoadDocuments(file = 'data/kataNegID.txt')[0]]\n",
    "\n",
    "print(len(Pos), len(Neg))\n",
    "print(Pos[:10],'\\n', Neg[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagaimana jika mau melakukannya dengan model klasifikasi (supervised learning) lain seperti modul sebelumnya?\n",
    "(e.g. SVM, NN, DT, k-NN, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text Classification : independent variable\n",
    "d1 = 'Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
    "d2 = 'Belajar NLP dan Text Mining ternyata seru banget'\n",
    "d3 = 'Palembang agak mendung hari ini'\n",
    "d4 =  'Sudah lumayan lama tukang Bakso belum lewat'\n",
    "d5 = 'Aduh ga banget makan Mie Ayam pakai kecap, please deh'\n",
    "d6 = 'Benci banget kalau melihat orang buang sampah sembarangan di jalan'\n",
    "d7 = 'Kalau liat orang ga taat aturan rasanya ingin ngegampar aja'\n",
    "d8 = 'Nikmatnya meniti jalan jalan penuh romansa di tengah kota bernuansa pendidikan'\n",
    "d9 = 'kemajuan bangsa ini ada pada kegigihan masyarakat dalam belajar dan bekerja'\n",
    "D = [d1,d2,d3,d4,d5,d6,d7,d8,d9]\n",
    "'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependent variable, misal 0=positif, 1=netral, 2=negatif\n",
    "Class = [0,0,1,1,2,2,2,1,0]\n",
    "dic = {0:'positif', 1:'netral', 2:'negatif'}\n",
    "print([dic[c] for c in Class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bentuk VSM-nya seperti kemarin (skip preprocessing)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "vsm = vectorizer.fit_transform(D)\n",
    "\n",
    "print(vsm.shape)\n",
    "str(vectorizer.vocabulary_)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakukan klasifikasi (misal dengan SVM)\n",
    "dSVM = svm.SVC(kernel='linear')\n",
    "sen = dSVM.fit(vsm, Class).predict(vsm)\n",
    "print(accuracy_score(Class, sen))\n",
    "# Memakai seluruh training data karena sampel yang sangat kecil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better yet, gunakan vocabulary-based VSM dan-atau Post-Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Supplementary\">Supplementary</h2>\n",
    "\n",
    "<p>* Negasi suatu kata bukan berarti memiliki sentimen kebalikannya. Misal &quot;jelek&quot; dan &quot;tidak jelek&quot; (terrible vs not terrible).</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/negation_sentiments.png\" /></p>\n",
    "\n",
    "<p>[*]. Zhu, X., Guo, H., Mohammad, S., &amp; Kiritchenko, S. (2014). An empirical study on the effect of negation words on sentiment. In&nbsp;<i>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>&nbsp;(Vol. 1, pp. 304-313).</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Makna positive/negative atau pro/kontra subjective (bias) terhadap user.\n",
    "* StopWords removal in general is a bad idea\n",
    "* learn the lingo in your topic, sentiment expressions are different across fields, languages, and regions.\n",
    "* Sarcasm perlu konteks untuk di deteksi dengan tepat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Feature-Engineering/Extraction\">Feature Engineering/Extraction</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Ketimbang pemilihan model yang optimal, beberapa literature sudah melaporkan bahwa feature engineering/extraction lebih efektif [1].</li>\n",
    "\t<li>Selain itu, pendekatan semantic dalam FE juga lebih plausible untuk dilakukan.</li>\n",
    "\t<li>Tabel berikut adalah contoh FE yang bisa dilakukan spesifik terhadap model SA.</li>\n",
    "\t<li><img alt=\"\" src=\"images/SA_Analysis_Features.png\" style=\"width: 544px; height: 425px;\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catatan penting di Sentimen Analysis:\n",
    "\n",
    "1. Hati-hati melakukan stopwords dan frequency filtering.\n",
    "2. Menggunakan Post-Tag Adjective dan-atau vocabulary-based VSM dapat membantu model sentiment analysis\n",
    "3. feature engineering lebih penting ketimbang model yang kompleks.\n",
    "4. Corpus bisa jadi harus berbeda untuk setiap konteks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latihan:\n",
    "\n",
    "1. Lakukan Sentimen Analysis pada data Tweet menggunakan himpunan Lexicon diatas.\n",
    "2. Membuat piechart proporsi sentimen\n",
    "3. Memisahkan seluruh Tweet yang pos, neg, dan netral\n",
    "4. Apakah hasilnya cukup baik? Apa yang bisa dilakukan untuk memperbaiki hasil?\n",
    "5. Export masing-masing ke text file untuk dianalisa lebih lanjut (misal text analytics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopId, lemmaId = tau.LoadStopWords(lang='id') \n",
    "slangFixId = tau.loadCorpus(file = 'data/slang.dic', sep=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2 = tau.loadTweets(file='data/Tweets.json')\n",
    "D = [t['full_text'] for t in T2] # Tweet hasil crawling\n",
    "data = []\n",
    "for i, d in tqdm(enumerate(D)):\n",
    "    doc = tau.cleanText(d, fix=slangFixId, symbols_remove = True, min_charLen = 3, max_charLen = 12, fixTag= True, fixMix=True)\n",
    "    data.append(doc)\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jawaban di cell ini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> End of Module 08\n",
    "\n",
    "<hr />\n",
    "<p><img alt=\"\" src=\"images/6_SocMed_cartoon.png\" /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pos = [t.strip() for t in tau.LoadDocuments(file = 'data/kataPosID.txt')[0]]\n",
    "Neg = [t.strip() for t in tau.LoadDocuments(file = 'data/kataNegID.txt')[0]]\n",
    "\n",
    "print(Pos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vsm = vectorizer.fit_transform(data)\n",
    "All_words = list(vectorizer.vocabulary_.keys())\n",
    "All_lexicon = set(Pos+Neg)\n",
    "Net = [kata for kata in All_words if kata not in All_lexicon]\n",
    "\n",
    "Net[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bentukClassifier(Pos, Neg, Net) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediksi = []\n",
    "for tweet in tqdm(data):\n",
    "    prediksi.append(prediksiSentiment(tweet,model))\n",
    "    \n",
    "plot = sns.countplot(prediksi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes: disini saya menggunakan \"D\" dan bukan \"data\"\n",
    "negatif_data = [d for d,p in zip(D, prediksi) if p=='negatif']\n",
    "\n",
    "negatif_data[:7]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
